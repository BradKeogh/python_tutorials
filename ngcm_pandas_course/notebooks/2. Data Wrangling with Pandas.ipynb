{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of this notebook is based off [Chris' previous work](https://github.com/fonnesbeck/statistical-analysis-python-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from seaborn import set_style\n",
    "set_style(\"darkgrid\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "pd.set_option(\"max_rows\", 10)\n",
    "np.set_printoptions(suppress=True)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(\"ignore\", SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date/time types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides conveniences for working with dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Range of Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = pd.date_range(\"1/1/2015\", periods=75, freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-01-01', '2015-01-02', '2015-01-03', '2015-01-04',\n",
       "               '2015-01-05', '2015-01-06', '2015-01-07', '2015-01-08',\n",
       "               '2015-01-09', '2015-01-10', '2015-01-11', '2015-01-12',\n",
       "               '2015-01-13', '2015-01-14', '2015-01-15', '2015-01-16',\n",
       "               '2015-01-17', '2015-01-18', '2015-01-19', '2015-01-20',\n",
       "               '2015-01-21', '2015-01-22', '2015-01-23', '2015-01-24',\n",
       "               '2015-01-25', '2015-01-26', '2015-01-27', '2015-01-28',\n",
       "               '2015-01-29', '2015-01-30', '2015-01-31', '2015-02-01',\n",
       "               '2015-02-02', '2015-02-03', '2015-02-04', '2015-02-05',\n",
       "               '2015-02-06', '2015-02-07', '2015-02-08', '2015-02-09',\n",
       "               '2015-02-10', '2015-02-11', '2015-02-12', '2015-02-13',\n",
       "               '2015-02-14', '2015-02-15', '2015-02-16', '2015-02-17',\n",
       "               '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21',\n",
       "               '2015-02-22', '2015-02-23', '2015-02-24', '2015-02-25',\n",
       "               '2015-02-26', '2015-02-27', '2015-02-28', '2015-03-01',\n",
       "               '2015-03-02', '2015-03-03', '2015-03-04', '2015-03-05',\n",
       "               '2015-03-06', '2015-03-07', '2015-03-08', '2015-03-09',\n",
       "               '2015-03-10', '2015-03-11', '2015-03-12', '2015-03-13',\n",
       "               '2015-03-14', '2015-03-15', '2015-03-16'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = pd.Series(np.random.randn(75), index=dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-01-01    0.734241\n",
       "2015-01-02    0.342056\n",
       "2015-01-03    1.607594\n",
       "2015-01-04    2.195226\n",
       "2015-01-05    0.745854\n",
       "                ...   \n",
       "2015-03-12   -1.189899\n",
       "2015-03-13   -0.594539\n",
       "2015-03-14    0.538563\n",
       "2015-03-15    0.513822\n",
       "2015-03-16   -0.111454\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index    datetime64[ns]\n",
       "0               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reset_index().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can index a DatetimeIndex with strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-01-01    0.734241\n",
       "2015-01-02    0.342056\n",
       "2015-01-03    1.607594\n",
       "2015-01-04    2.195226\n",
       "2015-01-05    0.745854\n",
       "                ...   \n",
       "2015-01-11   -1.127565\n",
       "2015-01-12   -0.914404\n",
       "2015-01-13   -0.044061\n",
       "2015-01-14   -0.184197\n",
       "2015-01-15   -0.208373\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.ix[\"2015-01-01\":\"2015-01-15\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DatetimeIndex supports partial string indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-01-01    0.734241\n",
       "2015-01-02    0.342056\n",
       "2015-01-03    1.607594\n",
       "2015-01-04    2.195226\n",
       "2015-01-05    0.745854\n",
       "                ...   \n",
       "2015-01-27   -0.233405\n",
       "2015-01-28   -1.532863\n",
       "2015-01-29   -0.771478\n",
       "2015-01-30   -1.994366\n",
       "2015-01-31   -0.772294\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[\"2015-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can resample to a lower frequency, specifying how to aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: FutureWarning: how in .resample() is deprecated\n",
      "the new syntax is .resample(...).mean()\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2015-01-31    0.016436\n",
       "2015-02-28   -0.043390\n",
       "2015-03-31   -0.119821\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.resample(\"M\", how=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or go to a higher frequency, optionally specifying how to fill in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-01-01 00:00:00    0.734241\n",
       "2015-01-01 01:00:00    0.734241\n",
       "2015-01-01 02:00:00    0.734241\n",
       "2015-01-01 03:00:00    0.734241\n",
       "2015-01-01 04:00:00    0.734241\n",
       "                         ...   \n",
       "2015-03-15 20:00:00    0.513822\n",
       "2015-03-15 21:00:00    0.513822\n",
       "2015-03-15 22:00:00    0.513822\n",
       "2015-03-15 23:00:00    0.513822\n",
       "2015-03-16 00:00:00   -0.111454\n",
       "Freq: H, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.asfreq(\"H\", method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are convenience methods to lag and lead time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-01-01    0.734241\n",
       "2015-01-02    0.342056\n",
       "2015-01-03    1.607594\n",
       "2015-01-04    2.195226\n",
       "2015-01-05    0.745854\n",
       "                ...   \n",
       "2015-03-12   -1.189899\n",
       "2015-03-13   -0.594539\n",
       "2015-03-14    0.538563\n",
       "2015-03-15    0.513822\n",
       "2015-03-16   -0.111454\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-01-01         NaN\n",
       "2015-01-02    0.734241\n",
       "2015-01-03    0.342056\n",
       "2015-01-04    1.607594\n",
       "2015-01-05    2.195226\n",
       "                ...   \n",
       "2015-03-12    0.532170\n",
       "2015-03-13   -1.189899\n",
       "2015-03-14   -0.594539\n",
       "2015-03-15    0.538563\n",
       "2015-03-16    0.513822\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015-01-01    0.342056\n",
       "2015-01-02    1.607594\n",
       "2015-01-03    2.195226\n",
       "2015-01-04    0.745854\n",
       "2015-01-05    1.177917\n",
       "                ...   \n",
       "2015-03-12   -0.594539\n",
       "2015-03-13    0.538563\n",
       "2015-03-14    0.513822\n",
       "2015-03-15   -0.111454\n",
       "2015-03-16         NaN\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preserve the original data, you must specify the `freq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014-12-31    0.734241\n",
       "2015-01-01    0.342056\n",
       "2015-01-02    1.607594\n",
       "2015-01-03    2.195226\n",
       "2015-01-04    0.745854\n",
       "                ...   \n",
       "2015-03-11   -1.189899\n",
       "2015-03-12   -0.594539\n",
       "2015-03-13    0.538563\n",
       "2015-03-14    0.513822\n",
       "2015-03-15   -0.111454\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shift(-1, freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamps vs Periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas provides a few data types building on [`np.datetime64`](http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html)\n",
    "* Timestamps are a point in time\n",
    "* A sequences of Timestamps is a DatetimeIndex as we saw above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-01 00:00:00')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Timestamp(\"2015-1-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periods represent a span of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2015-01', 'M')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pd.Period(\"Jan-2015\")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MonthEnd>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2015Q1', 'Q-DEC')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pd.Period(\"2015Q1\")\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quarterly frequency is defined by when the fiscal year ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2015-01-01', 'D')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.asfreq('D', 'start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2015-03-31', 'D')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.asfreq('D', 'end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeriodIndex(['2010Q1', '2010Q2', '2010Q3', '2010Q4'], dtype='int64', freq='Q-DEC')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.PeriodIndex(start=\"2010Q1\", periods=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeriodIndex(['2010Q1', '2010Q2', '2010Q3', '2010Q4', '2011Q1', '2011Q2',\n",
       "             '2011Q3', '2011Q4', '2012Q1', '2012Q2', '2012Q3', '2012Q4',\n",
       "             '2013Q1', '2013Q2', '2013Q3', '2013Q4', '2014Q1', '2014Q2'],\n",
       "            dtype='int64', freq='Q-DEC')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.period_range(\"2010Q1\", \"2014Q2\", freq=\"Q\")\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PeriodIndex also supports partial string indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010Q1    1.374366\n",
       "2010Q2    0.477038\n",
       "2010Q3   -1.482556\n",
       "2010Q4    1.831602\n",
       "Freq: Q-DEC, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py = pd.Series(np.random.randn(len(dates)), index=dates)\n",
    "py['2010']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PeriodIndex provides the same conveniences as DatetimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009Q4    1.374366\n",
       "2010Q1    0.477038\n",
       "2010Q2   -1.482556\n",
       "2010Q3    1.831602\n",
       "2010Q4    0.089316\n",
       "            ...   \n",
       "2013Q1    0.246312\n",
       "2013Q2    1.477446\n",
       "2013Q3   -0.634172\n",
       "2013Q4   -0.945151\n",
       "2014Q1   -0.725473\n",
       "Freq: Q-DEC, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.shift(-1, freq='Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\utils\\dir2.py:65: FutureWarning: .resample() is now a deferred operation\n",
      "use .resample(...).mean() instead of .resample(...)\n",
      "  canary = getattr(obj, '_ipython_canary_method_should_not_exist_', None)\n",
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\utils\\dir2.py:74: FutureWarning: .resample() is now a deferred operation\n",
      "use .resample(...).mean() instead of .resample(...)\n",
      "  m = getattr(obj, name, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeriodIndexResampler [freq=<MonthEnd>, axis=0, closed=right, label=right, convention=start, base=0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.resample('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can manipulate `Period` objects using arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2015Q3', 'Q-DEC')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Period(\"2015Q1\") + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2014Q3', 'Q-DEC')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Period(\"2015Q1\") - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Timestamp objects, you need to use an [`Offset`](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot add integral value to Timestamp without offset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-034b7cff2d51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2015-1-1\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mpandas\\tslib.pyx\u001b[0m in \u001b[0;36mpandas.tslib._Timestamp.__add__ (pandas\\tslib.c:20118)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot add integral value to Timestamp without offset."
     ]
    }
   ],
   "source": [
    "pd.Timestamp(\"2015-1-1\") + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offests can be added / subtracted to / from a datetime object to obtain a shifted date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Timestamp(\"2015-1-1\") + pd.DateOffset(months=2, days=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subclasses of DateOffset implement custom date increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.datetime.now() + pd.datetools.Day()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offsets can be multiplied by an integer (positive or negative) so that the increment will be applied multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = pd.Timestamp(\"1-1-2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d + pd.datetools.BDay() * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use parameterized Offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d + pd.datetools.BDay(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offsets have a rollforward and rollback methods for moving a date forward or backward to the next or previous \"offset date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offset = pd.datetools.MonthEnd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offset.rollforward(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timedeltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Timedeltas](http://pandas.pydata.org/pandas-docs/dev/timedeltas.html) are differences in times, expressed in difference units, e.g. days, hours, minutes, seconds. \n",
    "* They can be both positive and negative.\n",
    "* pandas `Timedelta` is compatible with that of Python's `datetime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can construct a Timedelta object in many ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Timedelta(\"2 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an integer with a unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Timedelta(2, \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a DateOffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Timedelta(pd.datetools.Second(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a `datetime.timedelta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "pd.Timedelta(timedelta(days=2, seconds=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pd.date_range(\"1-1-2014\", \"1-31-2014\", freq=\"D\") + pd.datetools.Hour(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y - pd.Timedelta(\"12 hours 1 minute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "Pandas has time-series aware plotting capabilities. E.g., outside of the the Notebook, pan and zoom update the tick labels in a smart way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pd.Series(np.random.randn(50).cumsum(), \n",
    "              index=pd.date_range(\"1-1-2014\", periods=50, freq=\"D\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling and Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also provides a number of convenience functions for working on rolling or moving windows of time series through a common interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pd.rolling_<TAB>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.rolling_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', \n",
    "                                                          periods=1000))\n",
    "ts = ts.cumsum()\n",
    "\n",
    "rolling_mean = pd.rolling_mean(ts, window=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = ts.plot(style='r--', figsize=(10, 10))\n",
    "rolling_mean.plot(style='b', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Use `pandas.io.data.DataReader` to download daily stock information for `GOOG`. Re-sample the closing price to the monthly average and compute the centered rolling yearly mean. Plot it vs. the monthly average as we did above. **NOTE:** `DataReader` is deprecated and the [pandas-datareader](https://github.com/pydata/pandas-datareader) package should be used in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.io import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = data.DataReader(name=\"GOOG\", data_source=\"google\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will manipulate data collected from ocean-going vessels on the eastern seaboard. Vessel operations are monitored using the Automatic Identification System (AIS), a safety at sea navigation technology which vessels are required to maintain and that uses transponders to transmit very high frequency (VHF) radio signals containing static information including ship name, call sign, and country of origin, as well as dynamic information unique to a particular voyage such as vessel location, heading, and speed.\n",
    "\n",
    "The International Maritime Organization’s (IMO) International Convention for the Safety of Life at Sea requires functioning AIS capabilities on all vessels 300 gross tons or greater and the US Coast Guard requires AIS on nearly all vessels sailing in U.S. waters. The Coast Guard has established a national network of AIS receivers that provides coverage of nearly all U.S. waters. AIS signals are transmitted several times each minute and the network is capable of handling thousands of reports per minute and updates as often as every two seconds. Therefore, a typical voyage in our study might include the transmission of hundreds or thousands of AIS encoded signals. This provides a rich source of spatial data that includes both spatial and temporal information.\n",
    "\n",
    "For our purposes, we will use summarized data that describes the transit of a given vessel through a particular administrative area. The data includes the start and end time of the transit segment, as well as information about the speed of the vessel, how far it travelled, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit = pd.read_csv(\"../data/AIS/transit_segments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dates specified as an `object` dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's convert the dates to something we can work with, using `to_datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(transit.st_time.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you prefer day first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(transit.st_time.head(), dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas handles missing dates with a special `NaT` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.to_datetime([None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and convert them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit[\"st_time\"] = pd.to_datetime(transit.st_time)\n",
    "transit[\"end_time\"] = pd.to_datetime(transit.end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime types have a special `dt` accessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.st_time.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.st_time.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and joining DataFrame objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a peak at the transit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.seg_length.plot(kind=\"hist\", bins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A few long trips make this difficult to read\n",
    "* Let's look at a transformation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.log(transit.seg_length).plot(kind=\"hist\", bins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the vessel transit information as we need it, we may want a little more information regarding the vessels themselves. In the data/AIS folder there is a second table that contains information about each of the ships that traveled the segments in the segments table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vessels = pd.read_csv(\"../data/AIS/vessel_information.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vessels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vessels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at each ship type (that doesn't contain `/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vessels.ix[~vessels.type.str.contains(\"/\"), \"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vessels.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Several ships in the vessels data have traveled multiple segments as we would expect\n",
    "* Matching the names in the transit data to the vessels data is thus a many-to-one match\n",
    "* We can combine these two datasets using the `merge` method\n",
    "* Let's look at a trivial example of a merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generator aside**\n",
    "\n",
    "Generators in Python allow you to create a function that behaves like an iterator. I.e., a for loop. They are memory efficient since you do not need to keep the whole object in memory. Most things are generators by default in Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_range(n):\n",
    "    \"\"\"\n",
    "    This function returns a generator that counts from 0 to `n'\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        yield i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = func_range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in counter:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now consumed this generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in counter:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a list, we have to ask for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = list(func_range(10))\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the merge example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = list(range(3)) + list(range(3))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(dict(id=range(4), \n",
    "                        age=np.random.randint(18, 31, size=4)))\n",
    "\n",
    "df2 = pd.DataFrame(dict(id=idx, \n",
    "                        score=np.random.random(size=6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, pandas identifies like-named columns to merge on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.merge(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Also notice that id = 3 was dropped from the result\n",
    "* By default, pandas does an **inner join** on a merge\n",
    "  * This is the set intersection of the key(s)\n",
    "* We can change this using the `how` keyword and specifying \"other\" for an **outer join**\n",
    "  * This is the set union of the key(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.merge(df2, how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use set operations to identify common keys beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vessels.columns.intersection(transit.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is a many-to-one merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vessels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transit.merge(vessels).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explicitly specify one or more keys to merge on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta = transit.merge(vessels, on=\"mmsi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the `merge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Watch out**, when merging on columns, indices are discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = pd.DataFrame(np.random.randn(25, 2), \n",
    "                 index=pd.date_range('1/1/2015', periods=25))\n",
    "A[2] = np.repeat(list('abcde'), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = pd.DataFrame(np.random.randn(5, 2))\n",
    "B[2] = list('abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A.merge(B, on=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Join is like merge, but it works on the indices\n",
    "* The same could be achieved with merge and the `left_index` and `right_index` keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.set_index('mmsi', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit.join(vessels.set_index('mmsi')).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another common operation is appending data row-wise or column-wise to an existing dataset\n",
    "* We can use the `concat` function for this\n",
    "* Let's import two microbiome datasets, each consisting of counts of microorganisms from a particular patient. \n",
    "* We will use the first column of each dataset as the index.\n",
    "* The index is the unique biological classification of each organism, beginning with domain, phylum, class, and for some organisms, going all the way down to the genus level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb1 = pd.read_excel('../data/microbiome/MID1.xls', index_col=0, \n",
    "                    header=None, names=[\"Taxon\", \"Counts\"])\n",
    "mb2 = pd.read_excel('../data/microbiome/MID2.xls', index_col=0, \n",
    "                    header=None, names=[\"Taxon\", \"Counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb1.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb2.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate on the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((mb1, mb2), axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or concatenate column-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((mb1, mb2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the inner join while concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat((mb1, mb2), axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an outer join and select the values from the first if present in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb1.combine_first(mb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a hierarchical index based on keys identifying the original tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat([mb1, mb2], keys=['patient1', 'patient2']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "In the data/microbiome subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10th file that describes the content of each. Write code that imports each of the data spreadsheets and combines them into a single DataFrame, adding the identifying information from the metadata spreadsheet as columns in the combined DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEC Individual Contributions Data\n",
    "\n",
    "The individual contributions file contains each contribution from an individual to a federal committee if the contribution was at least \\$200. The upper limit in 2012 w as \\$2500.\n",
    "\n",
    "A 25% sub-sample of the data is provided for you in the `../data` directory, but if you want the full dataset to play with later, you can download the original data [here](http://www.fec.gov/disclosurep/pnational.do). Click 2012 and export contributor data.\n",
    "\n",
    "[Column Information](ftp://ftp.fec.gov/FEC/Presidential_Map/2012/DATA_DICTIONARIES/CONTRIBUTOR_FORMAT.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CMTE_ID             COMMITTEE ID                \n",
    "CAND_ID             CANDIDATE ID                \n",
    "CAND_NM             CANDIDATE NAME              \n",
    "CONTBR_NM           CONTRIBUTOR NAME           \n",
    "CONTBR_CITY         CONTRIBUTOR CITY            \n",
    "CONTBR_ST           CONTRIBUTOR STATE           \n",
    "CONTBR_ZIP          CONTRIBUTOR ZIP CODE            \n",
    "CONTBR_EMPLOYER     CONTRIBUTOR EMPLOYER            \n",
    "CONTBR_OCCUPATION   CONTRIBUTOR OCCUPATION          \n",
    "CONTB_RECEIPT_AMT   CONTRIBUTION RECEIPT AMOUNT     \n",
    "CONTB_RECEIPT_DT    CONTRIBUTION RECEIPT DATE          \n",
    "RECEIPT_DESC        RECEIPT DESCRIPTION         \n",
    "MEMO_CD             MEMO CODE               \n",
    "MEMO_TEXT           MEMO TEXT               \n",
    "FORM_TP             FORM TYPE               \n",
    "FILE_NUM            FILE NUMBER             \n",
    "TRAN_ID             TRANSACTION ID              \n",
    "ELECTION_TP         ELECTION TYPE/PRIMARY GENERAL INDICATOR \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fec = pd.read_csv(\"../data/fec_subsample.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename a few columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.rename(columns=dict(contbr_city='city',\n",
    "                        contbr_st='state',\n",
    "                        contbr_zip='zip',\n",
    "                        contbr_employer='employer',\n",
    "                        contb_receipt_amt='amount',\n",
    "                        contb_receipt_dt='date',\n",
    "                        contbr_occupation='occupation',\n",
    "                        contbr_nm='name',\n",
    "                        cand_nm='candidate'\n",
    "                        ), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fec['date'] = pd.to_datetime(fec.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text data operations\n",
    "\n",
    "Text fields have a `str` accessor that provide vectorized string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.election_tp.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some data cleaning using `regex`. Regular expressions are used to do pattern matching on strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.search(\"\\w+@[\\w.]+\", \"Email: jon@thewall.com; Birthday: 1-1-1970\").group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.search(\"\\d{1,2}[-/]\\d{1,2}[-/]\\d{4}\", \"Email: jon@thewall.com; Birthday: 1-1-1970\").group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common to use these operations to provide boolean indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `na=False` to denote that NaNs should not be preserved but turned into False matches for boolean indexing to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.election_tp.str.match(\"P$\", na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What primary elections do we have in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.ix[fec.election_tp.str.match(\"P$\", na=False)].date.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are they all in 2012?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = fec.election_tp.str.match(\"P$\", na=False)\n",
    "fec.ix[idx].date.dt.year.isin([2012]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct the typo we noticed above using the `replace` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fec['election_tp'] = fec.election_tp.str.replace('P2102', 'P2012')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's restrict our attention to the 2012 general election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec = fec.ix[fec.election_tp.str.match('G2012', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your index isn't meaningful, optionally you may reset it after re-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fec.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace method (not just for string data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_replace = {'33': 'FL', \n",
    "              '46': 'IN',\n",
    "              '48': 'MI',\n",
    "              '49': 'MI',\n",
    "              '7': 'NJ',\n",
    "              '77': 'LA',\n",
    "              '8': 'NJ',\n",
    "              '84': 'UT',\n",
    "              '91': 'CA'}\n",
    "\n",
    "fec.replace(to_replace=dict(state=to_replace), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "\n",
    "* **Splitting** the data into groups\n",
    "* **Applying** a function to each group separately\n",
    "* **Combining** the results into a new data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Abstractly, we use a mapping from labels to group names\n",
    "* Mapping can be\n",
    "  * Python function\n",
    "  * A conformable list of group numbers\n",
    "  * A dict mapping labels to groups\n",
    "  * A column name\n",
    "  * A list of any of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping on a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.groupby(\"state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping on multiple columnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.groupby(['city', 'state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping with a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_by_month(index):\n",
    "    return index.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back a `GroupBy` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper = fec.set_index('date').groupby(group_by_month)\n",
    "grouper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groups` attribute is a dictionary mapping the unique groups to the axis labels that define them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(grouper.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.groups.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupBy with MultiIndex using one of the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.set_index(['state', 'city']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.set_index(['state', 'city']).groupby(level='state').groups.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do attribute lookup column selection on GroupBy objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate through GroupBy objects much like through a dictionary's items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, group in grouper:\n",
    "    print(idx)\n",
    "    # print(group.head())\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a single group from a GroupBy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "june = grouper.get_group(6)\n",
    "print(june.amount.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply step\n",
    "\n",
    "* A number of operations can be performed as part of the apply step\n",
    "* **Aggregation**\n",
    "  * E.g., group sums or means, sizes and counts\n",
    "* **Transformation**\n",
    "  * Changes that preserve the index. E.g., standardizing data or filling NaNs by group.\n",
    "* **Filtration**\n",
    "  * Discarding groups based on some criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"max_rows\", 12):\n",
    "    print(grouper.amount.aggregate(np.sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the size of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.amount.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Other aggregation functions include\n",
    "\n",
    "    `mean, sum, size, count, std, var, sem, describe, first, last, nth, min,` and `max`\n",
    "   \n",
    "* These functions are all `nan`-aware and will use the `bottleneck` library if installed\n",
    "  * Bottleneck is used extensively in pandas such as in the statistical functions\n",
    "  * It is a set of specialized cython routines that are especially fast when dealing with arrays that have nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice that it automatically dropped the non-numeric columns, not exactly what we want with `file_num`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can use the `add_suffix` method to give the resulting column names that reflect the transformation\n",
    "* Use `to_frame` to put the result back to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.amount.mean().to_frame().add_suffix(\"_mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alternatively, use a list to get a DataFrame in the first place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper[['amount']].mean().add_suffix(\"_mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing custom or multiple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.amount.aggregate([np.mean, np.std, len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.amount.aggregate({'average': np.mean, 'count': len})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different functions to different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_frequent(X):\n",
    "    return X.to_frame().groupby(X.name).size().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"max_rows\", 12):\n",
    "    print(grouper.aggregate(dict(state=most_frequent,\n",
    "                                 amount=np.max)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations preserve the original index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from scipy.stats import zscore\n",
    "except:\n",
    "    def zscore(x):\n",
    "        return (x - x.mean())/x.std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Watch out** when working with pandas vs. numpy, pandas adjusts the degrees of freedom to give unbiased estimates whereas numpy does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series([1, 2, 3, 4]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed = grouper.amount.transform(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed.groupby(group_by_month).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed.groupby(group_by_month).std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `filter` method allows us to return a subset of the original groups\n",
    "* It takes a function that when applied to a group, return True or False\n",
    "* Suppose we want to look only at months where the average donation is above the overall (positive donation) median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "median = fec.ix[fec.amount > 0].amount.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.amount.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.amount.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.filter(lambda x : x.ix[x.amount > 0].amount.mean() > median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter to the 50 states plus DC using `filter`. We'll see another way to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_codes = {'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', \n",
    "               'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', \n",
    "               'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', \n",
    "               'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', \n",
    "               'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', \n",
    "               'WY'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_states = fec.groupby(\"state\").filter(lambda x: x.state.iloc[0] in state_codes)\n",
    "set(fec_states.state.unique()).difference(state_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del fec_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you need something different than aggregate and transform can provide. You can use apply for these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(group):\n",
    "    return group.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouper.amount.apply(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercises **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one of the following to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above we grouped by months, ignoring the year. Use a `groupby` to group by month and year to see the total amount contributed *for each candidate*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This data contains refund information. Using the same GroupBy object you create above, how many refunds are there per month by party? (Barack Obama was the Democratic Party candidate. Jill Stein was the Green Party candidate. Gary Earl Johnson was the Libertarian Party candidate. Everyone else was a Republican.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the top 5 occupations by amount contributed to each candidate by candidate. Look at both by amount and number of contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping DataFrame objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivot method allows a DataFrame to be transformed easily between long and wide formats in the same way as a pivot table is created in a spreadsheet. \n",
    "\n",
    "It takes three arguments: index, columns and values, corresponding to the DataFrame index (the row headers), columns and cell values, respectively.\n",
    "\n",
    "Let's look at the contributions by candidate across states over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fec_amounts = fec[[\"state\", \"date\", \"candidate\", \"amount\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is in stacked format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_amounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to look at time series of the contributions to each candidate aggregated over the days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fec_amounts = fec_amounts.groupby([\"state\", \"date\", \"candidate\"], \n",
    "                                  as_index=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can aggregate over the states to look at totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fec_totals = fec_amounts.groupby([\"date\", \"candidate\"], \n",
    "                                 as_index=False).amount.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_totals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot tables allow us to reshape the data so that we can look at the candidates as columns and the dates as the unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_totals = fec_totals.pivot(index='date', \n",
    "                              columns='candidate', \n",
    "                              values='amount')\n",
    "fec_totals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample to monthly net donation amounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_totals = fec_totals.resample(\"M\", how='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure it's sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_totals = fec_totals.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"max_rows\", 20):\n",
    "    print(fec_totals.sum().sort(inplace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_totals.sum().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_totals.sum().idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top candidates by fundraising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = fec_totals.sum().sort(inplace=False).index[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_totals[candidates].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = fec_totals[candidates].cumsum().plot(figsize=(10, 10))\n",
    "ax.axvline(x=pd.Timestamp('6-23-2012'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the data in wide format with (candidate, state) MultiIndex and sorted monthly totals as columns\n",
    "\n",
    "                         1/2012 2/2012 3/2012\n",
    "    (Candidate, State)\n",
    "    \n",
    "**Hint**: Have a look at `DataFrame.pivot_table`   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Un)stacking data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First put the data into a multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_monthly = fec_amounts.groupby([\"state\", \"date\", \"candidate\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Unstack** turn the specified levels of the MultiIndex into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fec_monthly.unstack(level=['state', 'candidate']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have a DatetimeIndex and can resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fec_monthly = fec_monthly.unstack(level=['state', \n",
    "                                         'candidate']).resample('M', how='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Stack** is the inverse of unstack\n",
    "* We can use it to go from 'wide' formats to 'long' formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_monthly.stack(level=[\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These functions are intelligent about handling missing data\n",
    "* If we want to preserve the full indices, we use `dropna=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_monthly.stack(level=['state'], dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go from wide to long format using the `melt` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta = pd.DataFrame({'first' : ['John', 'Mary'],\n",
    "                    'last' : ['Doe', 'Bo'],\n",
    "                    'height' : [5.5, 6.0],\n",
    "                    'weight' : [130, 150]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID variables uniquely index the rows and the columns are stacked to a single column with an identifier column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.melt(dta, id_vars=[\"first\", \"last\"], var_name=\"quantity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval using NumExpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Expression evaluation of Series and DataFrame\n",
    "* To benefit from using `eval`, you *must* have [numexpr](https://github.com/pydata/numexpr) installed\n",
    "* The larger the DataFrame and the larger the expression, the larger the speed-ups you will see\n",
    "  * Typically bigger than the cache size of your CPU\n",
    "* Typical speed-ups can be 2x-4x to as much as 20x\n",
    "\n",
    "#### NumExpr\n",
    "\n",
    "* Fast numerical expression evaluator for NumPy\n",
    "* Speed up operations on arrays (like \"3*a+4*b\")\n",
    "* Avoids temporary arrays and uses less memory than doing the same calculation in Python.\n",
    "  * Smart use of the cache\n",
    "  * Smart chunking\n",
    "  * Multi-threaded to help with problems that are not memory-bounded (e.g. those using transcendental functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval\n",
    "\n",
    "\n",
    "* Arithmetic operations except for the left shift (<<) and right shift (>>) operators, e.g., `df + 2 * pi / s ** 4 % 42 - the_golden_ratio`\n",
    "* Comparison operations, including chained comparisons, e.g., 2 < df < df2\n",
    "* Boolean operations, e.g., df < df2 and df3 < df4 or not df_bool\n",
    "* list and tuple literals, e.g., [1, 2] or (1, 2)\n",
    "* Attribute access, e.g., df.a\n",
    "* Subscript expressions, e.g., df[0]\n",
    "* Simple variable evaluation, e.g., pd.eval('df') (this is not very useful)\n",
    "\n",
    "\n",
    "* This Python syntax is not allowed:\n",
    "\n",
    "* Expressions\n",
    "  * Function calls\n",
    "  * is/is not operations\n",
    "  * if expressions\n",
    "  * lambda expressions\n",
    "  * list/set/dict comprehensions\n",
    "  * Literal dict and set expressions\n",
    "  * yield expressions\n",
    "  * Generator expressions\n",
    "  * Boolean expressions consisting of only scalar values\n",
    "* Statements\n",
    "  * Neither simple nor compound statements are allowed. This includes things like for, while, and if."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['x{}'.format(i) for i in range(1, 31)]\n",
    "dta = pd.DataFrame(np.random.randn(10000, 30), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expr = \" + \".join(columns)\n",
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.eval(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `eval` to do one assignment is allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dta.eval(\"y = \" + expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To distinguish between variable names in the DataFrame and local variables, prepend an `@` to the local variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.eval(\"@x1 + \" + expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `python` and `pandas` parsers to express queries using a more intuitive syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.eval(\"(x1 > 0) & (x2 > 0) & (x3 > 0)\", parser=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas parser is even more lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.eval(\"x1 > 0 & x2 > 0 & x3 > 0\", parser=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.eval(\"x1 > 0 and x2 > 0 and x3 > 0\", parser=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large DataFrames 1-10mm rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./eval-perf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Small\" DataFrames (15k-20k rows) are faster in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"./eval-perf-small.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Indexing and Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New in `0.16.1`, Series and DataFrames provide a `sample` method for selecting random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %run load_fec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.sample(n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify a percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.sample(frac=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set a seed using an integer or a `numpy.random.RandomState` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(100)\n",
    "fec.sample(frac=.1, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do boostrap (sampling with replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec_bs = fec.sample(frac=.1, replace=True, random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fec_bs.index.unique()) == fec_bs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del fec_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass sampling weights. Say we want a sample that has roughly the same amount of people from each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec = fec.dropna(subset=['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "populations = fec.groupby(\"state\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make populations conformable to `fec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "populations = populations.ix[fec.state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get inverse population weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = 1/populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_fec = fec.sample(frac=.1, weights=weights, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_fec.groupby(\"state\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more permutation and sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the `sample` method, we could use numpy to do permutation. \n",
    "\n",
    "Calling `np.random.permutation` with the number of observations produces a shuffled index of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.permutation(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.random.permutation(len(fec))\n",
    "idx = idx[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.take(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could shuffle the index inplace using `np.random.shuffle`. Recall, however, that indices are immutable, so we use an array representation accessed through `values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = fec.index.copy().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.ix[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on sampling methods later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scalar selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only need one value then use `at` for label based lookup and `iat` for integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.columns[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.iat[10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.index[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.at[27673, 'city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw one way to filter using `groupby` above. Using `isin` is another way. It allows you to find columns that have a value contained in a set in a performant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_codes = {'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', \n",
    "               'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', \n",
    "               'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', \n",
    "               'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', \n",
    "               'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', \n",
    "               'WY'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fec.state.isin(state_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall that boolean indexing returns a subset of the data. \n",
    "* To keep the same shape as the original data, use the `where` method \n",
    "* This means that where must be a condition on the entire DataFrame or Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = fec.amount\n",
    "fec.where(y == 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.where(y == 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dta = pd.DataFrame(np.random.randn(1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.where(dta < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mask` is the inverse of where. It masks the data where the condition is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dta.mask(dta < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query method allows subset selection via an expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fec.query(\"amount == 2500 | amount == -2500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query method is smart about using indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rng.randint(100 / 2, size=(100, 2)), \n",
    "                  columns=list('bc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.query(\"index > 10 and b < c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using a named index query falls back on the index if a column name is not found. This also works for a MultiIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.index.name = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.query(\"a > 10 and b < c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `in`/`notin` instead of the `isin` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.query(\"a in [2, 3]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way of expression is powerfully succinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.query(\"a not in b and b < c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.ix[~df.index.isin(df.b) & (df.b < df.c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare DataFrames with the `equals` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.ix[~df.index.isin(df.b) & (df.b < df.c)].equals(df.query(\"a not in b and b < c\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `../data/weather_nyc.csv` contains some data on temperatures in New York City from 1995 to 2015. Let's clean up this data to get it into a format we will use later for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data\n",
    "2. Create 3 DataFrames\n",
    "   1. Create a DataFrame called `past_stats` with a (month, day) index that contains for every year < 2014\n",
    "      1. The daily low (\"lower\")\n",
    "      2. The daily high (\"upper\")\n",
    "      3. The average temperature (\"avg\")\n",
    "      4. The standard error of the temperature (\"std_err\") (See below)\n",
    "      5. Upper confidence interval for the average (\"ci_upper\") (See below)\n",
    "      6. Lower confidence interval for the average (\"ci_lower\") (See below)\n",
    "   2. Create a Series called `present_lows` with (month, day) index that contains *only* those days in 2014 for which the observed temperature was lower than the past lows.\n",
    "      1. The values should be the observed low temperature.\n",
    "   3. Create a Series called `present_highs` with (month, day) index that contains *only* those days in 2014 for which the observed temperature was higher than the past high.\n",
    "   \n",
    "The standard error of the temperature is given\n",
    "\n",
    "$$\\epsilon_x=\\frac{\\sigma_x}{\\sqrt(n)}$$\n",
    "\n",
    "where $\\sigma_x$ is the standard deviation of `x`. The confidence interals are\n",
    "\n",
    "$$(\\mu_x - 1.96 \\epsilon_x,\\mbox{ } \\mu_x + 1.96 \\epsilon_x)$$\n",
    "\n",
    "where $\\mu_x$ is the average temperature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Odds and Ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.util.testing.makeDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.util.testing.makeMissingDataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.util.testing.makeTimeSeries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you need to write unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.util.testing.makeDataFrame()\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.util.testing.assert_frame_equal(df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.util.testing.makeDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.util.testing.assert_frame_equal(df, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are also facilities for doing proper floating point comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.values[:, :] += 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.util.testing.assert_frame_equal(df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.util.testing.assert_frame_equal(df, df2, check_less_precise=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
